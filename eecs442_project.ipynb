{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4PiAxIJp1reB",
        "MPjtaORD1wRU",
        "WpXlpeIY10TD",
        "aRbAWC0A4lWF"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "phgoVxggTO0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea15a590-a465-465f-da9d-429059c783ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.13.0+cu116\n",
            "Torchvision Version:  0.14.0+cu116\n",
            "Using the GPU!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "  \n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")\n",
        "\n",
        "data_dir = \"./data_miniplaces_modified\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG Step 1 Build dataloaders"
      ],
      "metadata": {
        "id": "4PiAxIJp1reB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79TglcDw7-gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG Step 2 Data Augmentation"
      ],
      "metadata": {
        "id": "AEqDav7-74hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Rotate(deg=20):\n",
        "  def _rotate(img):\n",
        "    degree = np.random.randint(-deg, deg)\n",
        "        return rotate(img, degree, reshape=False)\n",
        "    return _rotate\n",
        "\n",
        "def Grayscale():\n",
        "    def _grayscale(img):\n",
        "        return np.stack((np.mean(img, axis=2), np.mean(img, axis=2), np.mean(img, axis=2)), axis=2)\n",
        "    return _grayscale\n",
        "\n",
        "def Flipping():\n",
        "    def _flipping(img):\n",
        "        return np.fliplr(img)\n",
        "    return _flipping\n",
        "\n",
        "def Zooming():\n",
        "    def _zooming(img):\n",
        "        return ndimage.zoom(img, 3.0)\n",
        "    return _zooming\n",
        "\n",
        "def augment(filename, transforms, n=1, original=True):\n",
        "    print(f\"Augmenting {filename}\")\n",
        "    img = imread(filename)\n",
        "    res = [img] if original else []\n",
        "    for i in range(n):\n",
        "        new = img\n",
        "        for transform in transforms:\n",
        "            new = transform(new)\n",
        "        res.append(new)\n",
        "    return res\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"Create augmented dataset.\"\"\"\n",
        "    reader = csv.DictReader(open(args.input, \"r\"), delimiter=\",\")\n",
        "    writer = csv.DictWriter(\n",
        "        open(f\"{args.datadir}/augmented_dogs.csv\", \"w\"),\n",
        "        fieldnames=[\"filename\", \"semantic_label\", \"partition\", \"numeric_label\", \"task\"],\n",
        "    )\n",
        "    augment_partitions = set(args.partitions)\n",
        "    augmentations = [Grayscale(), Rotate(), Flipping(), Zooming()]\n",
        "\n",
        "    writer.writeheader()\n",
        "    os.makedirs(f\"{args.datadir}/augmented/\", exist_ok=True)\n",
        "    for f in glob.glob(f\"{args.datadir}/augmented/*\"):\n",
        "        print(f\"Deleting {f}\")\n",
        "        os.remove(f)\n",
        "    for row in reader:\n",
        "        if row[\"partition\"] not in augment_partitions:\n",
        "            imwrite(\n",
        "                f\"{args.datadir}/augmented/{row['filename']}\",\n",
        "                imread(f\"{args.datadir}/images/{row['filename']}\"),\n",
        "            )\n",
        "            writer.writerow(row)\n",
        "            continue\n",
        "        imgs = augment(\n",
        "            f\"{args.datadir}/images/{row['filename']}\",\n",
        "            augmentations,\n",
        "            n=1,\n",
        "            original=True,\n",
        "            # original = False \n",
        "        )\n",
        "        for i, img in enumerate(imgs):\n",
        "            fname = f\"{row['filename'][:-4]}_aug_{i}.png\"\n",
        "            imwrite(f\"{args.datadir}/augmented/{fname}\", img)\n",
        "            writer.writerow(\n",
        "                {\n",
        "                    \"filename\": fname,\n",
        "                    \"semantic_label\": row[\"semantic_label\"],\n",
        "                    \"partition\": row[\"partition\"],\n",
        "                    \"numeric_label\": row[\"numeric_label\"],\n",
        "                    \"task\": row[\"task\"],\n",
        "                }\n",
        "            )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"input\", help=\"Path to input CSV file\")\n",
        "    parser.add_argument(\"datadir\", help=\"Data directory\", default=\"./data/\")\n",
        "    parser.add_argument(\n",
        "        \"-p\",\n",
        "        \"--partitions\",\n",
        "        nargs=\"+\",\n",
        "        help=\"Partitions (train|val|test|challenge|none)+ to apply augmentations to. Defaults to train\",\n",
        "        default=[\"train\"],\n",
        "    )\n",
        "    main(parser.parse_args(sys.argv[1:]))\n"
      ],
      "metadata": {
        "id": "NL5hORaszLN3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "02ef4e78-fafb-4554-ce9e-42d22dd021a5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-a2025771ec16>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    return rotate(img, degree, reshape=False)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG Step 3 Build MiniVGG and MiniVGG-BN"
      ],
      "metadata": {
        "id": "MPjtaORD1wRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniVGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        cfgs = {'MiniVGG': [64, 'M', 128, 'M', 128, 128, 'M'], 'MiniVGG-BN': [64, 'M', 128, 'M', 128, 128, 'M']}\n",
        "        l = []\n",
        "        i=3\n",
        "        batch_norm=True\n",
        "        for c in cfgs['MiniVGG-BN']:\n",
        "            if c == 'M':    \n",
        "                l += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(i, c, kernel_size=3, padding=1) \n",
        "                if not batch_norm:\n",
        "                    l += [conv2d, nn.ReLU(inplace=True)] \n",
        "                else:\n",
        "                    l += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)] \n",
        "                i=c\n",
        "        features = torch.nn.Sequential(*l)\n",
        "\n",
        "        self.features = features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((5, 5))\n",
        "        self.classifier = nn.Sequential(nn.Linear(3200, 512), nn.ReLU(inplace=True), nn.Dropout(p=0.3, inplace=False), nn.Linear(512, 256), nn.ReLU(inplace=True), \n",
        "            nn.Dropout(p=0.3, inplace=False), nn.Linear(256, 100))\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu') \n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0) \n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0) \n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01) \n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        ##\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"This function defines the forward propagation for a batch of input examples, by\n",
        "            successively passing output of the previous layer as the input into the next layer (after applying\n",
        "            activation functions), and returning the final output as a torch.Tensor object\n",
        "\n",
        "            You may optionally use the x.shape variables below to resize/view the size of\n",
        "            the input matrix at different points of the forward pass\"\"\"\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        ## TODO: forward pass\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1) \n",
        "        x = self.classifier(x) \n",
        "        ##\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "0B14bS5h1ytj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG Step 4 Build training/validation loops"
      ],
      "metadata": {
        "id": "WpXlpeIY10TD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_optimizer(model):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model: NN to train\n",
        "\n",
        "    Returns:\n",
        "        optimizer: pytorch optmizer for updating the given model parameters.\n",
        "    \"\"\"\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "    return optimizer\n",
        "\n",
        "def get_loss():\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        criterion: pytorch loss. \n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return criterion"
      ],
      "metadata": {
        "id": "KEFOBUPc12QS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloaders, criterion, optimizer, save_dir = None, num_epochs=25, model_name='MiniVGG'):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model: The NN to train\n",
        "        dataloaders: A dictionary containing at least the keys \n",
        "                    'train','val' that maps to Pytorch data loaders for the dataset\n",
        "        criterion: The Loss function\n",
        "        optimizer: Pytroch optimizer. The algorithm to update weights \n",
        "        num_epochs: How many epochs to train for\n",
        "        save_dir: Where to save the best model weights that are found. Using None will not write anything to disk.\n",
        "\n",
        "    Returns:\n",
        "        model: The trained NN\n",
        "        tr_acc_history: list, training accuracy history. Recording freq: one epoch.\n",
        "        val_acc_history: list, validation accuracy history. Recording freq: one epoch.\n",
        "    \"\"\"\n",
        "\n",
        "    val_acc_history = []\n",
        "    tr_acc_history = []\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            # loss and number of correct prediction for the current batch\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            # TQDM has nice progress bars\n",
        "            for inputs, labels in tqdm(dataloaders[phase]):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = torch.max(outputs.data, 1)[1]\n",
        "\n",
        "                if phase == \"train\":\n",
        "                  loss.backward()\n",
        "                  optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "                # save the best model weights\n",
        "                # =========================================================== #\n",
        "                # IMPORTANT:\n",
        "                # Losing your connection to colab will lead to loss of trained \n",
        "                # weights.\n",
        "                # You should download the trained weights to your local machine. \n",
        "                # Later, you can load these weights directly without needing to \n",
        "                # train the neural networks again.\n",
        "                # =========================================================== #\n",
        "                if save_dir:\n",
        "                    torch.save(best_model_wts, os.path.join(save_dir, model_name + '.pth'))\n",
        "\n",
        "            # record the train/val accuracies\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "            else:\n",
        "                tr_acc_history.append(epoch_acc)\n",
        "                \n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    return model, tr_acc_history, val_acc_history\n"
      ],
      "metadata": {
        "id": "CQ1jbvbC4i02"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG Step 5 Measure top1 and top5 accuracies of MiniVGG and MiniVGG-BN"
      ],
      "metadata": {
        "id": "aRbAWC0A4lWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"\n",
        "    Computes the accuracy over the k top predictions for the specified values \n",
        "    of k.\n",
        "    \n",
        "    Args:\n",
        "        output: pytorch tensor, (batch_size x num_classes). Outputs of the \n",
        "                network for one batch.\n",
        "        target: pytorch tensor, (batch_size,). True labels for one batch.\n",
        "    \n",
        "    Returns:\n",
        "        res: list. Accuracies corresponding to topk[0], topk[1], ...\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def test(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    top1_acc = []\n",
        "    top5_acc = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            res = accuracy(outputs, labels, topk=(1, 5))\n",
        "\n",
        "            top1_acc.append(res[0] * len(outputs))\n",
        "            top5_acc.append(res[1] * len(outputs))\n",
        "\n",
        "    print('Top-1 accuracy {}%, Top-5 accuracy {}%'.format(sum(top1_acc).item()/10000, sum(top5_acc).item()/10000))"
      ],
      "metadata": {
        "id": "H7zGtXdn4sIR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### To pass the test, both networks should have Top-5 accuracy above 55% #####\n",
        "vgg_BN.load_state_dict(torch.load('./weights/MiniVGG-BN.pth'))\n",
        "vgg.load_state_dict(torch.load('./weights/MiniVGG.pth'))\n",
        "\n",
        "test(vgg_BN, dataloaders['test'])\n",
        "test(vgg, dataloaders['test'])"
      ],
      "metadata": {
        "id": "a3Q6P2U55GLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image classification KNN\n"
      ],
      "metadata": {
        "id": "jI-6Wtf4-X1T"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6btKZ_RzAjp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inception V2"
      ],
      "metadata": {
        "id": "RA9m0WZUAeOt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0DYKAjK-hzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}